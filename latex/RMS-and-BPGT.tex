\documentclass[11pt]{article}

\usepackage[numbers,sort&compress]{natbib}  
%% Daniel added this, should help citations look nicer. You may need to delete temp files and rebuild the latex document from a clean start.

\newcommand{\daniel}[1]{{\textbf{{\small{\color{magenta}DL}: #1{\color{magenta}$\circ$}}}}} 
\newcommand{\owen}[1]{\textbf{{\small{\color{red}OK}: #1{\color{red}$\circ$}}}} 
\newcommand{\ed}[1]{\textbf{{\small{\color{blue}ED}: #1{\color{blue}$\circ$}}}}

%\renewcommand{\daniel}[1]{}
%\renewcommand{\owen}[1]{}
%\renewcommand{\ed}[1]{}


% Use utf-8 encoding for foreign characters
\usepackage[utf8]{inputenc}
\usepackage{url}
% Setup for fullpage use
\usepackage{fullpage}
\usepackage{color}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

% Uncomment some of the following if you use the features
%
% Running Headers and footers
%\usepackage{fancyhdr}

% Multipart figures
%\usepackage{subfigure}

% More symbols
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{latexsym}

% Surround parts of graphics with box
\usepackage{boxedminipage}

% Package for including code in the document
\usepackage{listings}

% If you want to generate a toc for each chapter (use with book)
\usepackage{minitoc}

% This is now the recommended way for checking for PDFLaTeX:
\usepackage{ifpdf}

%\newif\ifpdf
%\ifx\pdfoutput\undefined
%\pdffalse % we are not running PDFLaTeX
%\else
%\pdfoutput=1 % we are running PDFLaTeX
%\pdftrue
%\fi

% \ifpdf
\usepackage[pdftex]{graphicx}
% \else
% \usepackage{graphicx}
% \fi

\title{RMS and Backpropagation for Neural Networks}
\author{Eduardo Gutarra}

% \date{2010--06--13}

\begin{document}
	
\ifpdf
\DeclareGraphicsExtensions{.pdf, .jpg, .tif}
\else
\DeclareGraphicsExtensions{.eps, .jpg}
\fi
	
\maketitle
	
\section{Introduction} % (fold)
\label{sec:introduction}

Artificial Neural networks are computational models that mimic the architecture the structure and/or functional aspects of biological
neural networks such as the human brain. Neural networks are made up of multiple processing elements called neurons. These neurons are
interconnected and work in parallel to perform various complex tasks. make predictions when similar patterns or new patterns are presented. Neural
networks have applications in natural speech recognition, something, something among others.

In this report we will examine two different learning algorithms for feedforward neural networks the back-propagation and the RMS
reductions.

\section{Feedforward Neural Networks} % (fold)
\label{sec:feedforward_neural_networks}

The feedforward network in which all neurons of adjacent layers are interconnected


% section feedforward_neural_networks (end)


% section introduction (end)


\section{Algorithms and Procedures} % (fold)
\label{sec:algorithms_and_procedures}

\subsection{Data Structure} % (fold)
\label{sub:data_structure}

% subsection data_structure (end)

\subsection{Algorithms} % (fold)
\label{sub:algorithms}

$E$ is a set of examples
$e$ is a single example
$I(e)$ set of inputs in a single example
$T(e)$ set of target outputs for a given example
$O(e)$ are the target outputs of the example
Each iteration of this loop we will call an epoch

\begin{algorithmic}
	\FOR {$e \in E$} 
		\STATE $O(e) \gets$ FeedForward($I(e)$)
		\STATE CalculateOutputDeltas($O(e)$, $T(e)$)
		\STATE CalculateInternalDeltas
		\STATE UpdateWeights
	\ENDFOR
\end{algorithmic}

\begin{algorithm}
{
\begin{algorithmic}[1]
\STATE \textbf{input:}  Unsorted table $t$ with $n$ rows and $c$ columns.
\STATE \textbf{output:} a sorted table
\STATE Form
$K$~different versions of $t$, sorted differently: $t^{(1)},t^{(2)},\dots, t^{(K)}$ 

\STATE $\beta \leftarrow $ empty list
\STATE pick an element in $t^{(1)}$ randomly, add it to $\beta$ and remove it from all $t^{(i)}$'s
\WHILE{$\mathrm{size}(\beta)<n$ }
\STATE let $r$ be the latest element added to $\beta$
\STATE Given $i\in \{1,2,\dots,K\}$, there are up to two neighbors in sorted order within list $t^{(i)}$; out of up to $2K$ such neighbors, pick a nearest neighbor $r'$ to $r$ in Hamming distance.
\STATE Add $r'$ to $\beta$ and remove it from all $t^{(i)}$'s
\ENDWHILE
\STATE \textbf{return} $\beta$
\end{algorithmic}
}
\caption{\label{algo:multiplelists}The \textsc{Multiple Lists} heuristic}

\end{algorithm}

\begin{algorithm}
{
	\textbf{Main algorithm:}
\begin{algorithmic}[1]
\STATE \textbf{input:} a table $t$ and a block size $p$
\STATE \textbf{output:} a sorted table
%\STATE $\alpha \leftarrow $the columns in non-decreasing order of cardinality
%\STATE $\beta \leftarrow $ empty list \owen{beta is unused}\daniel{leftover from a copy/paste?}
\STATE each value in each tuple is assigned a Boolean value (initially false) indicating whether the value is DC
\FOR{$k=1,2,\dots, c-1$}
%\STATE append column $k$ to list $\beta$
\STATE sort table $t$ %on the first $k$~columns %\owen{$\beta$, else $\beta$ unused} %the first $k$~columns 
using the DC order with parameter $k$ (see below)
\FOR{every run of identical values in column $k$}
\STATE Mark %$x-\lfloor x/p \rfloor p$~values 
                                       $x \bmod p$~values 
as DC where $x$ is the length of the run 
\ENDFOR
\ENDFOR
\STATE sort $t$ using the DC order with $k=c$
\STATE \textbf{return} $t$
\end{algorithmic}
}
%\hrule
\textbf{DC order:}
{
\begin{algorithmic}[1]
\STATE \textbf{input:} two $c$-tuples $x,y$, each component of each tuple might be marked as DC; a positive integer $k$ (we assume $k-1\leq c$)
\STATE \textbf{output:} 1,0,-1 depending on whether $x$ is greater, equal or smaller than $y$ 
\FOR{$i = 1,2,\dots,k-1$}
\IF{neither $x_i$ nor $y_i$ are DC}
\STATE \textbf{return} 1 if $x_i>y_i$ or -1 if $x_i<y_i$
\ELSIF{$y_i$ is DC}
\STATE \textbf{return} 1
\ELSIF{$x_i$ is DC}
\STATE \textbf{return} -1
\ENDIF
\ENDFOR
\FOR{$i = k,k+1,\dots,c$}
\STATE \textbf{return} 1 if $x_i>y_i$ , -1 if $x_i<y_i$
\ENDFOR
\STATE \textbf{return} 0 
\end{algorithmic}
}
\caption{\label{algo:dcsort}The \textsc{DCSort} heuristic %\daniel{I tried to clean up further the pseudocode.}%\daniel{Pseudocode was buggy as remarked by Owen. }
%\owen{wonder whether we should revisit our for loop pseudocode syntax, which
%suggests that $i$ could take values from the set in any order.}\daniel{Fixed.}
}

\end{algorithm}


% subsection algorithms (end)

\section{Procedure} % (fold)
\label{sec:procedure}

% section procedure (end)

% section algorithms_and_procedures (end)

\section{Results} % (fold)
\label{sec:results}

\subsection{Time Complexity} % (fold)
\label{sub:time_complexity}

To determine the number of operations performed in each epoch of the learning algorithm we count the weights between the layers of the
neural network. To do this we consider that the number of weights between two layers of neurons is the product of the number of neurons
of each layer. Therefore the total number of weights for the entire network is $\sum_{i=0}^{n-1}N_{i}N_{i+1}$ where $N_{i}$ is the
number of neurons in each layer, and $n$ is the total number of layers. For the backpropagation algorithm we obtained that the number
of operations in a single epoch depends on the number of weights and bias. Therefore it has linear complexity $O(n)$ with respect to
the number of neurons. In the RMS algorithm the number of operations in a single epoch of the RMS algorithm is squared because we run
the feedforward algorithm for each change we do on a single weight.

% subsection time_complexity (end)

% section results (end)

\section{Discussion} % (fold)
\label{sec:discussion}

% section discussion (end)

\section{Conclusion} % (fold)
\label{sec:conclusion}

% section conclusion (end)

\section{Experiments} % (fold)
\label{sec:experiments}

These are the experiments

% section experiments (end)
	
    
\bibliographystyle{plain}
\bibliography{../../bib/lemur}
\end{document} 